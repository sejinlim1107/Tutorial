{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv7_8cDjv5Cy"
      },
      "source": [
        "# MLP (MNIST, Tensorflow)\n",
        "In this tutorial, we will use MNIST data to practice Multi Layer Perceptron with Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j5cq24fKv5C2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du3_WmnQv5C5"
      },
      "source": [
        "# Collect MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILKFOUEJv5C5"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDehdhEkv5C6",
        "outputId": "f6dc113a-148d-432e-ac69-fa9d69e2cd01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIspB1JMv5C6"
      },
      "source": [
        "train data has **60000** samples  \n",
        "test data has **10000** samples   \n",
        "every data is **28 * 28** pixels  \n",
        "\n",
        "below image shows 28*28 pixel image sample for hand written number '0' from MNIST data.  \n",
        "MNIST is gray scale image [0 to 255] for hand written number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8-GLRSgv5C7"
      },
      "source": [
        "![0 from MNIST](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mnist_sample.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHGiZi6mv5C8"
      },
      "source": [
        "# Split train data into train and validation data\n",
        "Validation during training gives advantages below,  \n",
        "1) check if train goes well based on validation score  \n",
        "2) apply **early stopping** when validation score doesn't improve while train score goes up (overcome **overfitting**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmb744KVv5C8"
      },
      "outputs": [],
      "source": [
        "x_val  = x_train[50000:60000]\n",
        "x_train = x_train[0:50000]\n",
        "y_val  = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfMw8zyYv5C9",
        "outputId": "33cfb834-20eb-457d-8e1c-35e81462213d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data has 50000 samples\n",
            "every train data is 28 * 28 image\n"
          ]
        }
      ],
      "source": [
        "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
        "print(\"every train data is \" + str(x_train.shape[1]) \n",
        "      + \" * \" + str(x_train.shape[2]) + \" image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAUUrUKmv5C9",
        "outputId": "5bba7ccf-be62-40bb-d4b2-f6255aedd52d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation data has 10000 samples\n",
            "every train data is 28 * 28 image\n"
          ]
        }
      ],
      "source": [
        "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
        "print(\"every train data is \" + str(x_val.shape[1]) \n",
        "      + \" * \" + str(x_train.shape[2]) + \" image\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPxaZgB5v5C-"
      },
      "source": [
        "28 * 28 pixels has gray scale value from **0** to **255**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm2Zr521v5C-",
        "outputId": "bad49f91-c709-4a2c-f71e-40eb4003efc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ],
      "source": [
        "# gray scale 값 보기\n",
        "print(x_train[0][8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRWWxLXQv5C_"
      },
      "source": [
        "각각의 학습 데이터는 **0** 부터 **9** 까지 라벨을 갖고 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3plNrXcv5C_",
        "outputId": "e0989676-2178-4683-ed34-da9a95d04229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5 0 4 1 9 2 1 3 1]\n"
          ]
        }
      ],
      "source": [
        "# 첫번째부터 10번째까지 라벨 확인하기\n",
        "print(y_train[0:9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q9RaiWbv5C_"
      },
      "source": [
        "테스트 데이터는 총 **10000** 개\n",
        "모든 테스트 데이터는 **28 * 28** 크기의 이미지임  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHiX1fjEv5DA",
        "outputId": "4ba33742-0828-4aac-cf6a-dbc5a647660d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test data has 10000 samples\n",
            "every test data is 28 * 28 image\n"
          ]
        }
      ],
      "source": [
        "print(\"테스트 데이터의 총 개수 \" + str(x_test.shape[0]))\n",
        "print(\"모든 test data의 크기는 \" + str(x_test.shape[1]) \n",
        "      + \" * \" + str(x_test.shape[2]) + \" 인 이미지임\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY587z4Nv5DA"
      },
      "source": [
        "# Reshape\n",
        "In order to fully connect all pixels to hidden layer,  \n",
        "we will reshape (28, 28) into (28x28,1) shape.  \n",
        "It means we flatten row x column shape to an array having 28x28 (756) items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "ca0RcsYvv5DA",
        "outputId": "f1c9dfb4-efba-461b-a4e5-1c8b12fee7b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\" width=\"500\" height=\"400\"/>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\", width=500, height=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPG637Xhv5DB",
        "outputId": "cfd38202-b663-4dd9-dc8f-a7189a94a11f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train.reshape(50000, 784)\n",
        "x_val = x_val.reshape(10000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V9aGpg0v5DC",
        "outputId": "329351b6-caa1-4f12-c0b1-209a7c92b2ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0], dtype=uint8)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1EIUJ-v5DC"
      },
      "source": [
        "# Normalize data\n",
        "normalization usually helps faster learning speed, better performance  \n",
        "by reducing variance and giving same range to all input features.  \n",
        "since MNIST data set all input has 0 to 255, normalization only helps reducing variances.  \n",
        "it turned out normalization is better than standardization for MNIST data with my MLP architeture,    \n",
        "I believe this is because relu handles 0 differently on both feed forward and back propagation.  \n",
        "handling 0 differently is important for MNIST, since 1-255 means there is some hand written,  \n",
        "while 0 means no hand written on that pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sudeUWQbv5DD"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "gray_scale = 255\n",
        "x_train /= gray_scale\n",
        "x_val /= gray_scale\n",
        "x_test /= gray_scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCC9sHcFv5DD"
      },
      "source": [
        "# label to one hot encoding value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcOroWUyv5DD"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Onr7dsJfv5DD",
        "outputId": "5a1ae9a2-bc45-4df0-a02c-99deec2c7d65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2c0A86Nv5DE"
      },
      "outputs": [],
      "source": [
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQhFHK0xv5DE"
      },
      "outputs": [],
      "source": [
        "def mlp(x):\n",
        "    # hidden layer1\n",
        "    w1 = tf.Variable(tf.random_uniform([784,256]))\n",
        "    b1 = tf.Variable(tf.zeros([256]))\n",
        "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
        "    # hidden layer2\n",
        "    w2 = \n",
        "    b2 = \n",
        "    h2 = \n",
        "    # output layer\n",
        "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
        "    b3 = tf.Variable(tf.zeros([10]))\n",
        "    logits= tf.matmul(h2, w3) + b3\n",
        "    \n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIgaJv4Cv5DE"
      },
      "outputs": [],
      "source": [
        "logits = mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8j-pA5Fv5DF"
      },
      "outputs": [],
      "source": [
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    logits=logits, labels=y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-j65Xixv5DF"
      },
      "outputs": [],
      "source": [
        "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Bt_XiZv5DF",
        "outputId": "62b1f39b-3c0d-43d9-ed30-20e04115f32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, validation accuracy: 0.1064, loss: 9479.23924316406\n",
            "epoch: 1, validation accuracy: 0.7404, loss: 487.9066563034058\n",
            "epoch: 2, validation accuracy: 0.8683, loss: 20.24389074325562\n",
            "epoch: 3, validation accuracy: 0.8761, loss: 11.892984285354613\n",
            "epoch: 4, validation accuracy: 0.8858, loss: 9.276838760375973\n",
            "epoch: 5, validation accuracy: 0.8785, loss: 8.25918293952942\n",
            "epoch: 6, validation accuracy: 0.8832, loss: 7.402374687194823\n",
            "epoch: 7, validation accuracy: 0.906, loss: 6.622725062370303\n",
            "epoch: 8, validation accuracy: 0.9034, loss: 5.537717547416686\n",
            "epoch: 9, validation accuracy: 0.8971, loss: 4.807866144180299\n",
            "epoch: 10, validation accuracy: 0.8396, loss: 7.349521398544313\n",
            "epoch: 11, validation accuracy: 0.9066, loss: 6.607095966339113\n",
            "epoch: 12, validation accuracy: 0.8217, loss: 52.06143003463745\n",
            "epoch: 13, validation accuracy: 0.8922, loss: 15.170511302948\n",
            "epoch: 14, validation accuracy: 0.9016, loss: 6.205790314674376\n",
            "epoch: 15, validation accuracy: 0.9036, loss: 4.978821859359741\n",
            "epoch: 16, validation accuracy: 0.9083, loss: 4.454537310600279\n",
            "epoch: 17, validation accuracy: 0.9112, loss: 3.8163385868072504\n",
            "epoch: 18, validation accuracy: 0.9137, loss: 3.665376300811767\n",
            "epoch: 19, validation accuracy: 0.9171, loss: 3.8778756713867173\n",
            "epoch: 20, validation accuracy: 0.9159, loss: 3.3953911519050597\n",
            "epoch: 21, validation accuracy: 0.9175, loss: 3.1974300575256356\n",
            "epoch: 22, validation accuracy: 0.8809, loss: 4.378022892475128\n",
            "epoch: 23, validation accuracy: 0.8764, loss: 4.933798418045042\n",
            "epoch: 24, validation accuracy: 0.912, loss: 4.379148626327513\n",
            "epoch: 25, validation accuracy: 0.9154, loss: 3.86887104511261\n",
            "epoch: 26, validation accuracy: 0.9196, loss: 3.1193934822082525\n",
            "epoch: 27, validation accuracy: 0.9205, loss: 2.864556527137757\n",
            "epoch: 28, validation accuracy: 0.9095, loss: 2.6170078659057614\n",
            "epoch: 29, validation accuracy: 0.9123, loss: 2.499971570968628\n",
            "[Test Accuracy] : 0.9089\n"
          ]
        }
      ],
      "source": [
        "# initialize\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# train hyperparameters\n",
        "epoch_cnt = 10\n",
        "batch_size = 1000\n",
        "iteration = len(x_train) // batch_size\n",
        "\n",
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    for epoch in range(epoch_cnt):\n",
        "        avg_loss = 0.\n",
        "        start = 0; end = batch_size\n",
        "        \n",
        "        for i in range(iteration):\n",
        "            _, loss = sess.run([train_op, loss_op], \n",
        "                               feed_dict={x: x_train[start: end], y: y_train[start: end]})\n",
        "            start += batch_size; end += batch_size\n",
        "            # Compute average loss\n",
        "            avg_loss += loss / iteration\n",
        "            \n",
        "        # Validate model\n",
        "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
        "        # Calculate accuracy\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "        cur_val_acc = accuracy.eval({x: x_val, y: y_val})\n",
        "        print(\"epoch: \"+str(epoch)+\", validation accuracy: \" \n",
        "              + str(cur_val_acc) +', loss: '+str(avg_loss))\n",
        "    \n",
        "    # Test model\n",
        "    preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test}))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "MLP_MNIST_Tensorflow.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}